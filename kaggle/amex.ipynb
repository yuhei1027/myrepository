{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle/amex",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLaLeAQov+6tCys1FsakG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhei1027/myrepository/blob/main/kaggle/amex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ZUbwjYAMAD",
        "outputId": "8c7e1b5d-e43d-4252-87bc-ae47cef0ba34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# コンペデータを格納しているパスを指定ください\n",
        "PATH = \"../content/drive/MyDrive/kaggle/amex\""
      ],
      "metadata": {
        "id": "_RfGbKljASbX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import itertools\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "m4SO9fCvAU92"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "if os.path.exists(f'{PATH}/test_folder')!=True:\n",
        "  os.mkdir(f'{PATH}/test_folder')\n",
        "for customer_id,df in tqdm(test.groupby(\"customer_ID\")):\n",
        "  df=df.reset_index(drop=True)\n",
        "  df.to_csv(f\"{PATH}/test_folder/test_{customer_id}.csv\",index=False)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oI5Q6btga98l",
        "outputId": "fa2286f4-41b8-4248-86b1-239bf6fbfc40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif os.path.exists(f\\'{PATH}/test_folder\\')!=True:\\n  os.mkdir(f\\'{PATH}/test_folder\\')\\nfor customer_id,df in tqdm(test.groupby(\"customer_ID\")):\\n  df=df.reset_index(drop=True)\\n  df.to_csv(f\"{PATH}/test_folder/test_{customer_id}.csv\",index=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ｄｓｆ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "0-gjqKmOaHtF",
        "outputId": "57baebad-875f-412c-edb0-5b95bac3f6c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1ef9fbfc5bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mｄｓｆ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dsf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Get the difference\n",
        "# ====================================================\n",
        "def get_difference(data, num_features):\n",
        "    df1 = []\n",
        "    customer_ids = []\n",
        "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
        "        # Get the differences\n",
        "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
        "        # Append to lists\n",
        "        df1.append(diff_df1)\n",
        "        customer_ids.append(customer_id)\n",
        "    # Concatenate\n",
        "    df1 = np.concatenate(df1, axis = 0)\n",
        "    # Transform to dataframe\n",
        "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
        "    # Add customer id\n",
        "    df1['customer_ID'] = customer_ids\n",
        "    return df1\n",
        "\n",
        "# ====================================================\n",
        "# Read & preprocess data and save it to disk\n",
        "# ====================================================\n",
        "def read_preprocess_data():\n",
        "    #trainデータを取得\n",
        "    train = pd.read_parquet(f'{PATH}/train.parquet')\n",
        "    #customer_IDとS_2以外の列名を取得\n",
        "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
        "    #カテゴリカル特徴量\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\",\n",
        "    ]\n",
        "    #連続特徴量\n",
        "    num_features = [col for col in features if col not in cat_features]\n",
        "\n",
        "    \"\"\"\n",
        "    print('Starting training feature engineer...')\n",
        "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
        "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
        "    train_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
        "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
        "    train_cat_agg.reset_index(inplace = True)\n",
        "\n",
        "    #targetを取得\n",
        "    train_labels = pd.read_csv(f'{PATH}/train_labels.csv')\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
        "\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
        "\n",
        "    # Get the difference\n",
        "    train_diff = get_difference(train, num_features)\n",
        "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
        "    # Save files to disk\n",
        "    train.to_parquet(f'{PATH}train_fe.parquet')\n",
        "    del train_num_agg, train_cat_agg, train_diff,train\n",
        "    gc.collect()\n",
        "    \"\"\"\n",
        "\n",
        "    test = pd.read_parquet(f'{PATH}/test.parquet')\n",
        "\n",
        "    print('Starting test feature engineer...')\n",
        "    #test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
        "\n",
        "    for i,col in enumerate(['mean', 'std', 'min', 'max', 'last']):\n",
        "      if i==0:\n",
        "        test_num_agg=test.groupby(\"customer_ID\")[num_features[0]].agg(col).reset_index()\n",
        "      else:\n",
        "        test_num_agg=test_num_agg.merge(test.groupby(\"customer_ID\")[num_features[0]].agg(col).reset_index(), how = 'inner', on = 'customer_ID')\n",
        "\n",
        "\n",
        "    print(test_num_agg.head())\n",
        "    return 1\n",
        "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean'])\n",
        "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
        "    test_num_agg.reset_index(inplace = True)\n",
        "\n",
        "\n",
        "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
        "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
        "    test_cat_agg.reset_index(inplace = True)\n",
        "\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
        "\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
        "\n",
        "    # Get the difference\n",
        "    test_diff = get_difference(test, num_features)\n",
        "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
        "\n",
        "    # Save files to disk\n",
        "    test.to_parquet(f'{PATH}test_fe.parquet')\n",
        "\n",
        "    del test_num_agg, test_cat_agg, test_diff,test\n",
        "    gc.collect()\n",
        "\n",
        "    # Save files to disk\n",
        "    test.to_parquet(f'{PATH}test_fe.parquet')\n",
        "\n",
        "# Read & Preprocess Data\n",
        "read_preprocess_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fB3wsuAAZRd",
        "outputId": "dc5fa6cf-ec26-4fb4-f8a5-9517d018d05a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting test feature engineer...\n",
            "                                         customer_ID     P_2_x     P_2_y     P_2_x     P_2_y       P_2\n",
            "0  00000469ba478561f23a92a868bd366de6f6527a684c9a...  0.601387  0.020190  0.568930  0.631315  0.568930\n",
            "1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...  0.862166  0.031436  0.794469  0.913501  0.841177\n",
            "2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...  0.748955  0.061456  0.673112  0.835114  0.697522\n",
            "3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...  0.474728  0.028856  0.428457  0.514222  0.513186\n",
            "4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...  0.324100  0.049865  0.254478  0.425764  0.254478\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import itertools\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "\n",
        "# ====================================================\n",
        "# Configurations\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    input_dir = '/content/data/'\n",
        "    seed = 52\n",
        "    n_folds = 6\n",
        "    target = 'target'\n",
        "    boosting_type = 'dart'\n",
        "    metric = 'binary_logloss'\n",
        "\n",
        "# ====================================================\n",
        "# Seed everything\n",
        "# ====================================================\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# ====================================================\n",
        "# Read data\n",
        "# ====================================================\n",
        "def read_data():\n",
        "    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n",
        "    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n",
        "    return train, test\n",
        "\n",
        "# ====================================================\n",
        "# Amex metric\n",
        "# ====================================================\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.6 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "# ====================================================\n",
        "# LGBM amex metric\n",
        "# ====================================================\n",
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "# ====================================================\n",
        "# Train & Evaluate\n",
        "# ====================================================\n",
        "def train_and_evaluate(train, test):\n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get the difference between last and mean\n",
        "    num_cols = [col for col in train.columns if 'last' in col]\n",
        "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
        "    for col in num_cols:\n",
        "        try:\n",
        "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
        "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
        "        except:\n",
        "            pass\n",
        "    # Transform float64 and float32 to float16\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    for col in tqdm(num_cols):\n",
        "        train[col] = train[col].astype(np.float16)\n",
        "        test[col] = test[col].astype(np.float16)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': CFG.metric,\n",
        "        'boosting': CFG.boosting_type,\n",
        "        'seed': CFG.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.013,\n",
        "        'feature_fraction': 0.20,\n",
        "        'bagging_freq': 10.56,\n",
        "        'bagging_fraction': 0.55,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2,\n",
        "        'min_data_in_leaf': 100,\n",
        "        }\n",
        "    score =0\n",
        "    test=[]\n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "        params = params,\n",
        "        train_set = lgb_train,\n",
        "        num_boost_round = 11000,\n",
        "        valid_sets = [lgb_train, lgb_valid],\n",
        "        early_stopping_rounds = 1500,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model, f'/content/drive/MyDrive/Amex/Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        test_pred = model.predict(test[features])\n",
        "        test_predictions += test_pred / CFG.n_folds\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    # Compute out of folds metric\n",
        "        score = amex_metric(train[CFG.target], oof_predictions)\n",
        "        print(f'Our out of folds CV score is {score}')\n",
        "     \n",
        "    # Create a dataframe to store out of folds predictions\n",
        "        oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
        "        oof_df.to_csv(f'/content/drive/MyDrive/Amex/OOF/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "        test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "        test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "\n",
        "# seed_everything(CFG.seed)\n",
        "# train, test = read_data()\n",
        "# train_and_evaluate(train, test)\n",
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "        import os\n",
        "        import gc\n",
        "        import joblib\n",
        "        import random\n",
        "        import warnings\n",
        "        import itertools\n",
        "        import scipy as sp\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        from tqdm import tqdm\n",
        "        import xgboost as xgb\n",
        "        import lightgbm as lgb\n",
        "        warnings.filterwarnings('ignore')\n",
        "        from itertools import combinations\n",
        "        pd.set_option('display.width', 1000)\n",
        "        pd.set_option('display.max_rows', 500)\n",
        "        from catboost import CatBoostClassifier\n",
        "        pd.set_option('display.max_columns', 500)\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "        class CFG:\n",
        "            input_dir = '../input/amex-fe/'\n",
        "            seed = 52\n",
        "            n_folds = 6\n",
        "            target = 'target'\n",
        "\n",
        "        def seed_everything(seed):\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "        def read_data():\n",
        "            train = pd.read_parquet(CFG.input_dir + 'train_fe_plus_plus.parquet')\n",
        "            test = pd.read_parquet(CFG.input_dir + 'test_fe_plus_plus.parquet')\n",
        "            return train, test\n",
        "\n",
        "        def amex_metric(y_true, y_pred):\n",
        "            labels = np.transpose(np.array([y_true, y_pred]))\n",
        "            labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "            weights = np.where(labels[:,0]==0, 20, 1)\n",
        "            cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "            top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "            gini = [0,0]\n",
        "            for i in [1,0]:\n",
        "                labels = np.transpose(np.array([y_true, y_pred]))\n",
        "                labels = labels[labels[:, i].argsort()[::-1]]\n",
        "                weight = np.where(labels[:,0]==0, 20, 1)\n",
        "                weight_random = np.cumsum(weight / np.sum(weight))\n",
        "                total_pos = np.sum(labels[:, 0] *  weight)\n",
        "                cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "                lorentz = cum_pos_found / total_pos\n",
        "                gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "            return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "        g=0\n",
        "\n",
        "        def amex_metric_np(preds, target):\n",
        "            indices = np.argsort(preds)[::-1]\n",
        "            preds, target = preds[indices], target[indices]\n",
        "            weight = 20.0 - target * 19.0\n",
        "            cum_norm_weight = (weight / weight.sum()).cumsum()\n",
        "            four_pct_mask = cum_norm_weight <= 0.04\n",
        "            d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
        "            weighted_target = target * weight\n",
        "            lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
        "            gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
        "            n_pos = np.sum(target)\n",
        "            n_neg = target.shape[0] - n_pos\n",
        "            gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
        "            g = gini / gini_max\n",
        "            return 0.6 * (g + d)\n",
        "            N_FOLDS = 5\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=22)\n",
        "    y_oof = np.zeros(train_x.shape[0])\n",
        "    y_test = np.zeros(test.shape[0])\n",
        "    ix = 0\n",
        "    for train_ind, val_ind in skf.split(train_x, train_y):\n",
        "        print(f\"******* Fold {ix} ******* \")\n",
        "        tr_x, val_x = (\n",
        "            train_x.iloc[train_ind].reset_index(drop=True),\n",
        "            train_x.iloc[val_ind].reset_index(drop=True),\n",
        "    )\n",
        "        tr_y, val_y = (\n",
        "            train_y.iloc[train_ind].reset_index(drop=True),\n",
        "            train_y.iloc[val_ind].reset_index(drop=True),\n",
        "    )\n",
        "\n",
        "        clf = CatBoostClassifier(iterations=5000, random_state=22)\n",
        "        clf.fit(tr_x, tr_y, eval_set=[(val_x, val_y)], cat_features=cat_features,  verbose=100)\n",
        "        preds = clf.predict_proba(val_x)[:, 1]\n",
        "        y_oof[val_ind] = y_oof[val_ind] + preds\n",
        "\n",
        "        preds_test = clf.predict_proba(test)[:, 1]\n",
        "        y_test = y_test + preds_test / N_FOLDS\n",
        "        ix = ix + 1\n",
        "    y_pred = train_y.copy(deep=True)\n",
        "    y_pred = y_pred.rename(columns={\"target\": \"prediction\"})\n",
        "    y_pred[\"prediction\"] = y_oof\n",
        "    val_score = amex_metric(train_y, y_pred)\n",
        "    print(f\"Amex metric: {val_score}\")\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import itertools\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "\n",
        "# ====================================================\n",
        "# Configurations\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    input_dir = '/content/data/'\n",
        "    seed = 42\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "\n",
        "# ====================================================\n",
        "# Seed everything\n",
        "# ====================================================\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# ====================================================\n",
        "# Read data\n",
        "# ====================================================\n",
        "def read_data():\n",
        "    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n",
        "    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n",
        "    return train, test\n",
        "\n",
        "# ====================================================\n",
        "# Amex metric\n",
        "# ====================================================\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "# ====================================================\n",
        "# LGBM amex metric\n",
        "# ====================================================\n",
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "# ====================================================\n",
        "# Train & Evaluate\n",
        "# ====================================================\n",
        "def train_and_evaluate(train, test):\n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': \"binary_logloss\",\n",
        "        'boosting': 'dart',\n",
        "        'seed': CFG.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.20,\n",
        "        'bagging_freq': 10,\n",
        "        'bagging_fraction': 0.50,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2,\n",
        "        'min_data_in_leaf': 40\n",
        "        }\n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "            params = params,\n",
        "            train_set = lgb_train,\n",
        "            num_boost_round = 10500,\n",
        "            valid_sets = [lgb_train, lgb_valid],\n",
        "            early_stopping_rounds = 100,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model, f'/content/drive/MyDrive/Amex/Models/lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        test_pred = model.predict(test[features])\n",
        "        test_predictions += test_pred / CFG.n_folds\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    # Compute out of folds metric\n",
        "    score = amex_metric(train[CFG.target], oof_predictions)\n",
        "    print(f'Our out of folds CV score is {score}')\n",
        "    # Create a dataframe to store out of folds predictions\n",
        "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
        "    oof_df.to_csv(f'/content/drive/MyDrive/Amex/OOF/oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "    test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "\n",
        "    ensemble=VotingClassifier(estimators=[('Decision Tree',g ), ('Random Forest', score)], voting='soft', weights=[2,1]).fit(train_x,train_y)\n",
        "    print('The accuracy for DecisionTree and Random Forest is:',ensemble.score(test_x,test_y))\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_x, test_x, train_y, test_y = train_test_split(dataset.drop('target'),dataset['target'])\n",
        "    trained_model = random_forest_classifier(train_x, train_y)\n",
        "    predictions = trained_model.predict(test_x)\n",
        "        \n"
      ],
      "metadata": {
        "id": "nU8U03nIH6wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "boZPtC9iD922"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}