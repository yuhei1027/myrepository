{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhei1027/myrepository/blob/main/kaggle/amex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ZUbwjYAMAD",
        "outputId": "96d3371d-fe37-48e3-f0ca-332856a4bf6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RfGbKljASbX"
      },
      "outputs": [],
      "source": [
        "# コンペデータを格納しているパスを指定ください\n",
        "PATH = \"../content/drive/MyDrive/kaggle/amex\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4SO9fCvAU92"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import itertools\n",
        "from sklearn.ensemble import VotingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oI5Q6btga98l",
        "outputId": "8bd249e8-5cc5-4554-9f60-6f5f434c8119"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nif os.path.exists(f\\'{PATH}/test_folder\\')!=True:\\n  os.mkdir(f\\'{PATH}/test_folder\\')\\nfor customer_id,df in tqdm(test.groupby(\"customer_ID\")):\\n  df=df.reset_index(drop=True)\\n  df.to_csv(f\"{PATH}/test_folder/test_{customer_id}.csv\",index=False)\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "if os.path.exists(f'{PATH}/test_folder')!=True:\n",
        "  os.mkdir(f'{PATH}/test_folder')\n",
        "for customer_id,df in tqdm(test.groupby(\"customer_ID\")):\n",
        "  df=df.reset_index(drop=True)\n",
        "  df.to_csv(f\"{PATH}/test_folder/test_{customer_id}.csv\",index=False)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "0-gjqKmOaHtF",
        "outputId": "2f3fc347-b17f-473a-c14a-459d565a123b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1ef9fbfc5bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mｄｓｆ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dsf' is not defined"
          ]
        }
      ],
      "source": [
        "ｄｓｆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BysgxhlCkhHk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_fold(df,n_fold=5,save=True,f_name=\"train\"):\n",
        "  id_map={id:np.random.randint(n_fold) for id in df[\"customer_ID\"].unique()}\n",
        "  df[\"fold\"]=df[\"customer_ID\"].apply(lambda x:id_map[x])\n",
        "\n",
        "  if save:\n",
        "    for i in range(n_fold):\n",
        "      print(i)\n",
        "      tmp_df=df[df[\"fold\"]==i].reset_index(drop=True).copy()\n",
        "      tmp_df.to_parquet(f'{PATH}/{f_name}_{str(i)}.parquet',index=False)\n",
        "  \n",
        "  return df\n",
        "\n",
        "#test = pd.read_parquet(f'{PATH}/test.parquet')\n",
        "#test=make_fold(test,f_name=\"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fB3wsuAAZRd"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Get the difference\n",
        "# ====================================================\n",
        "\n",
        "def get_difference(data, num_features):\n",
        "    df1 = []\n",
        "    customer_ids = []\n",
        "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
        "        # Get the differences\n",
        "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
        "        # Append to lists\n",
        "        df1.append(diff_df1)\n",
        "        customer_ids.append(customer_id)\n",
        "    # Concatenate\n",
        "    df1 = np.concatenate(df1, axis = 0)\n",
        "    # Transform to dataframe\n",
        "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
        "    # Add customer id\n",
        "    df1['customer_ID'] = customer_ids\n",
        "    return df1\n",
        "\n",
        "# ====================================================\n",
        "# Read & preprocess data and save it to disk\n",
        "# ====================================================\n",
        "def read_preprocess_data():\n",
        "    #trainデータを取得\n",
        "    train = pd.read_parquet(f'{PATH}/train.parquet')\n",
        "    #customer_IDとS_2以外の列名を取得\n",
        "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
        "    #カテゴリカル特徴量\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\",\n",
        "    ]\n",
        "    #連続特徴量\n",
        "    num_features = [col for col in features if col not in cat_features]\n",
        "\n",
        "    \"\"\"\n",
        "    print('Starting training feature engineer...')\n",
        "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
        "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
        "    train_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
        "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
        "    train_cat_agg.reset_index(inplace = True)\n",
        "\n",
        "    #targetを取得\n",
        "    train_labels = pd.read_csv(f'{PATH}/train_labels.csv')\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
        "\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
        "\n",
        "    # Get the difference\n",
        "    train_diff = get_difference(train, num_features)\n",
        "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
        "    # Save files to disk\n",
        "    train.to_parquet(f'{PATH}train_fe.parquet')\n",
        "    del train_num_agg, train_cat_agg, train_diff,train\n",
        "    gc.collect()\n",
        "    \"\"\"\n",
        "\n",
        "    def df_groupby_agg(df,groupby_col,feature_cols,ops):\n",
        "      flag=True\n",
        "      for feature in feature_cols:\n",
        "        for op in ops:\n",
        "          tmp_df=df.groupby(groupby_col)[feature].agg(op)\n",
        "          tmp_df.columns=[feature+\"_\"+op]\n",
        "          \n",
        "          if flag:\n",
        "            flag=False\n",
        "            df_agg=tmp_df.reset_index()\n",
        "          else:\n",
        "            df_agg=df_agg.merge(tmp_df.reset_index(), how = 'inner', on = 'customer_ID')\n",
        "      return df_agg\n",
        "\n",
        "    for i in range(5):\n",
        "\n",
        "\n",
        "      test = pd.read_parquet(f'{PATH}/test_{str(i)}.parquet')\n",
        "      \n",
        "      print('Starting test feature engineer...')\n",
        "      test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
        "      test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
        "      test_num_agg.reset_index(inplace = True)\n",
        "\n",
        "\n",
        "      test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
        "      test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
        "      test_cat_agg.reset_index(inplace = True)\n",
        "\n",
        "      # Transform float64 columns to float32\n",
        "      cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
        "      for col in tqdm(cols):\n",
        "          test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
        "\n",
        "      # Transform int64 columns to int32\n",
        "      cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
        "      for col in tqdm(cols):\n",
        "          test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
        "\n",
        "      # Get the difference\n",
        "      test_diff = get_difference(test, num_features)\n",
        "      test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
        "\n",
        "      # Save files to disk\n",
        "      test.to_parquet(f'{PATH}/test_fe_{str(i)}.parquet')\n",
        "\n",
        "      del test_num_agg, test_cat_agg, test_diff,test\n",
        "      gc.collect()\n",
        "\n",
        "    test_all=[]\n",
        "    for i in range(5):\n",
        "      test = pd.read_parquet(f'{PATH}/test_fe_{str(i)}.parquet')\n",
        "      test_all.append(test)\n",
        "    test_all=pd.concat(test_all).reset_index(drop=True)\n",
        "    # Save files to disk\n",
        "    test_all.to_parquet(f'{PATH}/test_fe.parquet')\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read & Preprocess Data\n",
        "read_preprocess_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_uI4VjrZe4",
        "outputId": "ffca12e0-6670-4501-8e77-3d4afeb7b165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "202207202159\n"
          ]
        }
      ],
      "source": [
        "dt_now = datetime.datetime.now()+ datetime.timedelta(hours=9)\n",
        "print(dt_now.strftime('%Y%m%d%H%M'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743,
          "referenced_widgets": [
            "88add671e132428685a48a76a017c9f3",
            "72c66bc67d674da89e23f51bbbe2db14",
            "b229ea90d9fa4e2d9025b8750d6cbbdd",
            "81cd525a651543f7ae266fca02074f25",
            "d89ed24a7a6345a0a487fe5647428daa",
            "491ed9cf775948a39a024c94fb2d202c",
            "69708b343a74490cb920a3f7ea95a7c0",
            "82e34bf232f047e5adf73f3fded1290e",
            "d8a8db20969e430ba0855dff0adc9bf6",
            "539d811ba17d4f6693ce68d039a1c090",
            "e2b6615c23fc4753a5ac1aa1894c98f4"
          ]
        },
        "id": "0zCZni4j7SE5",
        "outputId": "8e94a1f7-4606-4701-de02-6458a55650b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88add671e132428685a48a76a017c9f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1080 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "--------------------------------------------------\n",
            "Training fold 0 with 1365 features...\n",
            "[500]\ttraining's binary_logloss: 0.336653\ttraining's amex_metric: 0.778097\tvalid_1's binary_logloss: 0.33851\tvalid_1's amex_metric: 0.772682\n",
            "[1000]\ttraining's binary_logloss: 0.245853\ttraining's amex_metric: 0.795392\tvalid_1's binary_logloss: 0.251162\tvalid_1's amex_metric: 0.781993\n",
            "Our fold 0 CV score is 0.7819926818654553\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 1 with 1365 features...\n",
            "[500]\ttraining's binary_logloss: 0.336215\ttraining's amex_metric: 0.780143\tvalid_1's binary_logloss: 0.339407\tvalid_1's amex_metric: 0.763627\n",
            "[1000]\ttraining's binary_logloss: 0.245234\ttraining's amex_metric: 0.796574\tvalid_1's binary_logloss: 0.252786\tvalid_1's amex_metric: 0.775025\n",
            "Our fold 1 CV score is 0.7750247981733211\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 2 with 1365 features...\n",
            "[500]\ttraining's binary_logloss: 0.336163\ttraining's amex_metric: 0.779169\tvalid_1's binary_logloss: 0.339851\tvalid_1's amex_metric: 0.767617\n",
            "[1000]\ttraining's binary_logloss: 0.245114\ttraining's amex_metric: 0.79596\tvalid_1's binary_logloss: 0.253119\tvalid_1's amex_metric: 0.777537\n",
            "Our fold 2 CV score is 0.7775368896239132\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 3 with 1365 features...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bc357d53277e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-bc357d53277e>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mearly_stopping_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mfeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_amex_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             )\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Save best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import itertools\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "\n",
        "# ====================================================\n",
        "# Configurations\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    input_dir = '/content/data/'\n",
        "    seed = 42\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "    boosting_type = 'dart'\n",
        "    metric = 'binary_logloss'\n",
        "\n",
        "# ====================================================\n",
        "# Seed everything\n",
        "# ====================================================\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# ====================================================\n",
        "# Read data\n",
        "# ====================================================\n",
        "def read_data():\n",
        "    train = pd.read_parquet(f'{PATH}/train_fe.parquet')\n",
        "    test = pd.read_parquet(f'{PATH}/test_fe.parquet')\n",
        "    return train, test\n",
        "\n",
        "# ====================================================\n",
        "# Amex metric\n",
        "# ====================================================\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "# ====================================================\n",
        "# LGBM amex metric\n",
        "# ====================================================\n",
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "# ====================================================\n",
        "# Train & Evaluate\n",
        "# ====================================================\n",
        "def train_and_evaluate(train, test):\n",
        "    \n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get the difference between last and mean\n",
        "    num_cols = [col for col in train.columns if 'last' in col]\n",
        "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
        "    for col in num_cols:\n",
        "        try:\n",
        "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
        "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
        "        except:\n",
        "            pass\n",
        "    # Transform float64 and float32 to float16\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    for col in tqdm(num_cols):\n",
        "        train[col] = train[col].astype(np.float16)\n",
        "        test[col] = test[col].astype(np.float16)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': CFG.metric,\n",
        "        'boosting': CFG.boosting_type,\n",
        "        'seed': CFG.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.20,\n",
        "        'bagging_freq': 10,\n",
        "        'bagging_fraction': 0.50,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2,\n",
        "        'min_data_in_leaf': 40,\n",
        "        }\n",
        "\n",
        "    dt_now = datetime.datetime.now()+ datetime.timedelta(hours=9)\n",
        "    dt_now=dt_now.strftime('%Y%m%d%H%M')\n",
        "    os.mkdir(f'{PATH}/Predictions/{dt_now}')\n",
        "\n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "            params = params,\n",
        "            train_set = lgb_train,\n",
        "            #num_boost_round = 10500,\n",
        "            num_boost_round = 2000\n",
        "            ,\n",
        "            valid_sets = [lgb_train, lgb_valid],\n",
        "            early_stopping_rounds = 1500,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model, f'{PATH}/Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        n_test=308207\n",
        "        test_pred=np.zeros([len(test),])\n",
        "        for i in range(3):\n",
        "          test_pred[n_test*i:n_test*(i+1)] += model.predict(test[features].iloc[n_test*i:n_test*(i+1)])\n",
        "        \n",
        "        test_predictions+=test_pred\n",
        "\n",
        "        test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_pred})\n",
        "        test_df.to_csv(f'{PATH}/Predictions/{dt_now}/test_lgbm_{CFG.boosting_type}_baseline_{fold}fold_seed{CFG.seed}.csv', index = False)\n",
        "        #test_pred = model.predict(test[features])\n",
        "        #test_predictions += test_pred / CFG.n_folds\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    test_predictions/=CFG.n_folds\n",
        "    # Compute out of folds metric\n",
        "    score = amex_metric(train[CFG.target], oof_predictions)\n",
        "    print(f'Our out of folds CV score is {score}')\n",
        "    # Create a dataframe to store out of folds predictions\n",
        "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
        "    oof_df.to_csv(f'{PATH}/OOF/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "    test_df.to_csv(f'{PATH}/Predictions/{dt_now}/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "    \n",
        "seed_everything(CFG.seed)\n",
        "train, test = read_data()\n",
        "train_and_evaluate(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geEBAXAu3QCp"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# Library\n",
        "# ====================================================\n",
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import itertools\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "\n",
        "# ====================================================\n",
        "# Configurations\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    input_dir = '../content/drive/MyDrive/kaggle/amex/'\n",
        "    seed = 52\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "    boosting_type = 'dart'\n",
        "    metric = 'binary_logloss'\n",
        "\n",
        "# ====================================================\n",
        "# Seed everything\n",
        "# ====================================================\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# ====================================================\n",
        "# Read data\n",
        "# ====================================================\n",
        "def read_data():\n",
        "    train = pd.read_parquet(CFG.input_dir + 'train_fe.parquet')\n",
        "    test = pd.read_parquet(CFG.input_dir + 'test_fe.parquet')\n",
        "    return train, test\n",
        "\n",
        "# ====================================================\n",
        "# Amex metric\n",
        "# ====================================================\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.6 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "# ====================================================\n",
        "# LGBM amex metric\n",
        "# ====================================================\n",
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "# ====================================================\n",
        "# Train & Evaluate\n",
        "# ====================================================\n",
        "def train_and_evaluate(train, test):\n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get the difference between last and mean\n",
        "    num_cols = [col for col in train.columns if 'last' in col]\n",
        "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
        "    for col in num_cols:\n",
        "        try:\n",
        "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
        "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
        "        except:\n",
        "            pass\n",
        "    # Transform float64 and float32 to float16\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    for col in tqdm(num_cols):\n",
        "        train[col] = train[col].astype(np.float16)\n",
        "        test[col] = test[col].astype(np.float16)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': CFG.metric,\n",
        "        'boosting': CFG.boosting_type,\n",
        "        'seed': CFG.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.013,\n",
        "        'feature_fraction': 0.20,\n",
        "        #'bagging_freq': 10.56,\n",
        "        'bagging_fraction': 0.55,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2,\n",
        "        'min_data_in_leaf': 100,\n",
        "        }\n",
        "    score =0\n",
        "   \n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "        params = params,\n",
        "        train_set = lgb_train,\n",
        "        #num_boost_round = 11000,\n",
        "        num_boost_round=1,\n",
        "        valid_sets = [lgb_train, lgb_valid],\n",
        "        early_stopping_rounds = 1500,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model, f'{PATH}/Models/lgbm_{CFG.boosting_type}_fold{fold}_seed{CFG.seed}.pkl')\n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        n_test=308207\n",
        "        for i in range(3):\n",
        "          test_predictions[n_test*i:n_test*(i+1)] += model.predict(test[features].iloc[n_test*i:n_test*(i+1)])\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    test_predictions/=CFG.n_folds\n",
        "    # Compute out of folds metric\n",
        "    score = amex_metric(train[CFG.target], oof_predictions)\n",
        "    print(f'Our out of folds CV score is {score}')\n",
        "     \n",
        "    # Create a dataframe to store out of folds predictions\n",
        "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
        "    oof_df.to_csv(f'{PATH}/OOF/oof_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "    test_df.to_csv(f'{PATH}/Predictions/test_lgbm_{CFG.boosting_type}_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "\n",
        "seed_everything(CFG.seed)\n",
        "train, test = read_data()\n",
        "\n",
        "\n",
        "train_and_evaluate(train, test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn0roQ9WtBP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import itertools\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "\n",
        "# ====================================================\n",
        "# Configurations\n",
        "# ====================================================\n",
        "class CFG:\n",
        "    input_dir = '/content/data/'\n",
        "    seed = 42\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "\n",
        "# ====================================================\n",
        "# Seed everything\n",
        "# ====================================================\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# ====================================================\n",
        "# Read data\n",
        "# ====================================================\n",
        "def read_data():\n",
        "    train = pd.read_parquet(f'{PATH}/train_fe.parquet')\n",
        "    test = pd.read_parquet(f'{PATH}/test_fe.parquet')\n",
        "    return train, test\n",
        "\n",
        "# ====================================================\n",
        "# Amex metric\n",
        "# ====================================================\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "# ====================================================\n",
        "# LGBM amex metric\n",
        "# ====================================================\n",
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "# ====================================================\n",
        "# Train & Evaluate\n",
        "# ====================================================\n",
        "def train_and_evaluate(train, test):\n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': \"binary_logloss\",\n",
        "        'boosting': 'dart',\n",
        "        'seed': CFG.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.20,\n",
        "        'bagging_freq': 10,\n",
        "        'bagging_fraction': 0.50,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2,\n",
        "        'min_data_in_leaf': 40\n",
        "        }\n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "            params = params,\n",
        "            train_set = lgb_train,\n",
        "            num_boost_round = 10500,\n",
        "            valid_sets = [lgb_train, lgb_valid],\n",
        "            early_stopping_rounds = 100,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model, f'{PATH}/Models/lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        test_pred = model.predict(test[features])\n",
        "        test_predictions += test_pred / CFG.n_folds\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    # Compute out of folds metric\n",
        "    score = amex_metric(train[CFG.target], oof_predictions)\n",
        "    print(f'Our out of folds CV score is {score}')\n",
        "    # Create a dataframe to store out of folds predictions\n",
        "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
        "    oof_df.to_csv(f'{PATH}/Amex/OOF/oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "    test_df.to_csv(f'{PATH}/Predictions/test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
        "\n",
        "seed_everything(CFG.seed)\n",
        "train, test = read_data()\n",
        "train_and_evaluate(train, test)\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "kaggle/amex",
      "provenance": [],
      "authorship_tag": "ABX9TyOz/kuHVj6pb0pJpJgGjEU0",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "491ed9cf775948a39a024c94fb2d202c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539d811ba17d4f6693ce68d039a1c090": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69708b343a74490cb920a3f7ea95a7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72c66bc67d674da89e23f51bbbe2db14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_491ed9cf775948a39a024c94fb2d202c",
            "placeholder": "​",
            "style": "IPY_MODEL_69708b343a74490cb920a3f7ea95a7c0",
            "value": "100%"
          }
        },
        "81cd525a651543f7ae266fca02074f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_539d811ba17d4f6693ce68d039a1c090",
            "placeholder": "​",
            "style": "IPY_MODEL_e2b6615c23fc4753a5ac1aa1894c98f4",
            "value": " 1080/1080 [10:14&lt;00:00, 67.57it/s]"
          }
        },
        "82e34bf232f047e5adf73f3fded1290e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88add671e132428685a48a76a017c9f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72c66bc67d674da89e23f51bbbe2db14",
              "IPY_MODEL_b229ea90d9fa4e2d9025b8750d6cbbdd",
              "IPY_MODEL_81cd525a651543f7ae266fca02074f25"
            ],
            "layout": "IPY_MODEL_d89ed24a7a6345a0a487fe5647428daa"
          }
        },
        "b229ea90d9fa4e2d9025b8750d6cbbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82e34bf232f047e5adf73f3fded1290e",
            "max": 1080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8a8db20969e430ba0855dff0adc9bf6",
            "value": 1080
          }
        },
        "d89ed24a7a6345a0a487fe5647428daa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a8db20969e430ba0855dff0adc9bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2b6615c23fc4753a5ac1aa1894c98f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}